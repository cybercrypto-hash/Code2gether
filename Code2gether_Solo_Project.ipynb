{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sources for project: \n",
        "\n",
        "\n",
        "\n",
        "Data Preprocessing: \n",
        "\n",
        "1. https://datatofish.com/list-to-dataframe/\n",
        "2. https://www.geeksforgeeks.org/get-first-n-records-of-a-pandas-dataframe/\n",
        "3. https://www.geeksforgeeks.org/read-a-zipped-file-as-a-pandas-dataframe/\n",
        "4. https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "\n",
        "Webscraping Sources for online tutorials:\n",
        "\n",
        "1. https://www.geeksforgeeks.org/python-web-scraping-tutorial/\n",
        "2. https://www.google.com/search?client=firefox-b-1-d&q=webscraping+with+python\n",
        "3. https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/\n",
        "\n",
        "\n",
        "Natural Language Processing Tutorials here: \n",
        "\n",
        "https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "\n"
      ],
      "metadata": {
        "id": "wECerDBHEiGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "iSc4rqVVLbbt",
        "outputId": "814887eb-aec8-487d-e0a3-1ddf9cb1ad5f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b80eea9cac52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#data_Needed = pd.read_csv(\"/content/twitterSlang.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;31m#print(data_Needed.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "import requests; from bs4 import BeautifulSoup;import csv\n",
        "import html5lib \n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# 2000's slang:\n",
        "# Dataset imported here if neccessary: \n",
        "\n",
        "\n",
        "\n",
        "#dataset = pd.read_csv(\"/content/slang_2010_tweets.csv\")\n",
        "#df1 = dataset.head()\n",
        "#df2 = dataset.head()\n",
        "#dataset_2 = pd.read_csv(\"/content/slang_2020_tweets.csv\")\n",
        "# Ended up being a poor datatset\n",
        "#data_Needed = pd.read_csv(\"/content/twitterSlang.csv\") \n",
        "\n",
        "print(dataset.head()); print(dataset_2.head()); #print(data_Needed.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" The above dataset was insufficient for what we needed. \n",
        "Use the files of slang from the 2010s and 2020s instead, please. \n",
        "\"\"\"\n",
        "\n",
        "#Webscraped data: \n",
        "\n",
        "\"\"\"URL = \"https://people.howstuffworks.com/53-slang-terms-by-decade.htm\"\n",
        "URL1 = \"https://www.babbel.com/en/magazine/2000s-slang\"# website goes here in quotes]\n",
        "requests = requests.get(URL1)\n",
        "quotes_saved_URL = [] # store our quotes here\n",
        "soup_content = BeautifulSoup(requests.content, 'html5lib' )\n",
        "table_used = soup_content.find('div', attr = {'id': 'all_quotes'})\n",
        "print(URL)\n",
        "for row in table_used.findAll('div', attrs = {'class':'col-6 col-lg-3 text-center margin-30px-bottom sm-margin-30px-top'}):\n",
        "    quote = {}\n",
        "    quote['theme'] = row.h5.text\n",
        "    quote['url'] = row.a['href']\n",
        "    quote['img'] = row.img['src']\n",
        "    quote['lines'] = row.img['alt'].split(\" #\")[0]\n",
        "    quote['author'] = row.img['alt'].split(\" #\")[1]\n",
        "    quotes_saved_URL.append(quote)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# The above dataset was insufficient for what we needed. Use this one instead, please. \n",
        "\n",
        "\n",
        "\n",
        "# Comparing the semantic similarity of words here: \n",
        "\n",
        "\"\"\" In this project, we are comparing the similarities between slang words from\n",
        "the 2020s period. \n",
        "\n",
        "If you have time compare the 2010s to the 2020s. \n",
        "\n",
        "I want to compare the beginning, middle and ending of words. In addition, I \n",
        "would also like to compare how similar definitions have changed if we have time.\n",
        "\n",
        "Make a similarity gauge in how many times words had a length of 5 letters or \n",
        "less and how many times they were over 5 letters. \n",
        "\n",
        "Implement a linear regresssion to show changes that occurred in a span\n",
        "of decade. \n",
        "\n",
        "\"\"\"\n",
        "# Let's see what we're working with here. Use the first 100 in both categories: \n",
        "\n",
        "print(dataset, \"This is the dataset from the 2020s\") \n",
        "print(dataset_2, \"This is the dataset from the 2010s:\", \"\\n\")\n",
        "\n",
        "# Concentrate on the first 100 of each of the two files. \n",
        "\n",
        "dataset_divided = dataset[['word']]\n",
        "dataset_2_divided = []\n",
        "\n",
        "#print(stopwords.words(\"english\")); print(\"\\n\")\n",
        "\n",
        "# Word tokenization and lemmatization: \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mL6MUUOFzPy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sources for project: \n",
        "\n",
        "\n",
        "\n",
        "Data Preprocessing: \n",
        "\n",
        "1. https://datatofish.com/list-to-dataframe/\n",
        "2. https://www.geeksforgeeks.org/get-first-n-records-of-a-pandas-dataframe/\n",
        "3. https://www.geeksforgeeks.org/read-a-zipped-file-as-a-pandas-dataframe/\n",
        "4. https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "\n",
        "Webscraping Sources for online tutorials:\n",
        "\n",
        "1. https://www.geeksforgeeks.org/python-web-scraping-tutorial/\n",
        "2. https://www.google.com/search?client=firefox-b-1-d&q=webscraping+with+python\n",
        "3. https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/\n",
        "\n",
        "\n",
        "Natural Language Processing Tutorials here: \n",
        "\n",
        "https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "\n"
      ],
      "metadata": {
        "id": "9Y7oAV_1UeEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Clean version of my project: \n",
        "\n",
        "\"\"\" \n",
        "\n",
        "In this project, we are comparing the similarities between slang words from\n",
        "the 2020s period. \n",
        "\n",
        "If you have time compare the 2010s to the 2020s. \n",
        "\n",
        "I want to compare the beginning, middle and ending of words. In addition, I \n",
        "would also like to compare how similar definitions have changed if we have time.\n",
        "\n",
        "Make a similarity gauge in how many times words had a length of 5 letters or \n",
        "less and how many times they were over 5 letters. \n",
        "\n",
        "Implement a linear regresssion to show changes that occurred in a span\n",
        "of decade. \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Imported relevant libraries: \n",
        "\n",
        "import requests; from bs4 import BeautifulSoup;import csv\n",
        "import html5lib \n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# NLTK library:\n",
        "\n",
        "import nltk\n",
        "#nltk.download()\n",
        "#from nltk.corpus import brown; brown.words()\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "import re\n",
        "\n",
        "# Sklearn library:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import time \n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "\n",
        "# Data set recreated here for easier use:\n",
        "\n",
        "# Used a list here and changed it to a dataframe: \n",
        "\n",
        "data_for_slang_2022s = [\"stan\", \"salty\", \"simp\",\"cheugy\", \"zennial\", \"snack\", \"sip tea\", \"sheesh\", \"slaps\", \"bussin\", \"snatched\", \"sliving\", \"boujee\", \"CEO\", \"sameshitting\", \"ick\", \"ate that\", \"Bible\", \"Rent-free\",\"Out of pocket\", \"caught in 4k\", \"G.O.A.T\"]\n",
        "data_frame_2022s = pd.DataFrame (data_for_slang_2022s, columns = ['words used']); print(\"Outputting data from 2022s\", data_frame_2022s)\n",
        "dataset_slang_2010s = [\"catfish\", \"bromance\", \"cougar\", \"sexting\", \"heart\", \"earworm\", \"helicopter parent\", \"selfie\", \"photobomb\", \"binge-watch\", \"first world problem\", \"deets\", \"vom\", \"humblebrag\", \"dad dancing\", \"tweet\",\"buzzworthy\", \"unfriend\", \"jeggings\", \"Meme\", \"emoji\",\"Freegan\" ]\n",
        "data_frame_2010s = pd.DataFrame (dataset_slang_2010s, columns = ['words used']); print(\"\\n\", \"outputting data 2010s\",data_frame_2010s, \"\\n\")\n",
        "final = [\"stan\", \"salty\", \"simp\",\"cheugy\", \"zennial\", \"snack\",\n",
        "         \"sip tea\", \"sheesh\", \"slaps\", \"bussin\", \"snatched\", \"sliving\", \"boujee\", \"CEO\", \"sameshitting\", \n",
        "         \"ick\", \"ate that\", \"Bible\", \"Rent-free\",\"Out of pocket\", \"caught in 4k\", \"G.O.A.T\", \"2nd list\", \n",
        "         \"catfish\", \"bromance\", \"cougar\", \"sexting\", \"heart\", \"earworm\", \"helicopter parent\", \"selfie\", \"photobomb\", \"binge-watch\", \"first world problem\", \"deets\", \"vom\", \n",
        "         \"humblebrag\", \"dad dancing\", \"tweet\",\"buzzworthy\", \"unfriend\", \"jeggings\", \"Meme\", \"emoji\",\"Freegan\"]\n",
        "\"\"\"finals = stan salty simp cheugy zennial snack\",\n",
        "         \"sip tea\", \"sheesh\", \"slaps\", \"bussin\", \"snatched\", \"sliving\", \"boujee\", \"CEO\", \"sameshitting\", \n",
        "         \"ick\", \"ate that\", \"Bible\", \"Rent-free\",\"Out of pocket\", \"caught in 4k\", \"G.O.A.T\", \"2nd list\", \n",
        "         \"catfish\", \"bromance\", \"cougar\", \"sexting\", \"heart\", \"earworm\", \"helicopter parent\", \"selfie\", \"photobomb\", \"binge-watch\", \"first world problem\", \"deets\", \"vom\", \n",
        "         \"humblebrag\", \"dad dancing\", \"tweet\",\"buzzworthy\", \"unfriend\", \"jeggings\", \"Meme\", \"emoji\",\"Freegan\"]\n",
        "]\n",
        "# Comparing the semantic similarity of words here: \n",
        "# Word tokenization and lemmatization here(not needed for this project):\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"stan: \", lemmatizer.lemmatize(\"stan\"))\"\"\"\n",
        "\n",
        "\"\"\" We first need to perform text preprocessing. Why? It needs to be in a \n",
        "format in order to compare the items.  \"\"\"\n",
        "\n",
        "# Convert text to a vector of numbers tfid might help. \n",
        "\n",
        "# Removing any s's or plural forms of words via the bag of words algorithm. \n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bag_of_words = count_vectorizer.fit_transform(data_for_slang_2022s)\n",
        "bag_of_words_2nd_version = count_vectorizer.fit_transform(dataset_slang_2010s)\n",
        "print(\"This is a vector of numbers \", bag_of_words, \"\\n\"); print(\"This is a vector of numbers \",bag_of_words_2nd_version)\n",
        "\n",
        "\"\"\"\n",
        "Bag of words as a panda data frame here and outputted to the screen.  \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Below, we have the parts of speech part of the program. \n",
        "It shows the different parts of speech and the odds of it showing up in a text. \n",
        "\n",
        "\"\"\"\n",
        "# Parts of Speech(POS) tagging: \n",
        "\n",
        "\"\"\"\n",
        "data_Trained = state_union.raw(data_for_slang_2022s)\n",
        "sample_data = state_union.raw(data_for_slang_2022s)\n",
        "\n",
        "data_Trained1 = state_union.raw(final)\n",
        "sample_data1 = state_union.raw(final)\n",
        "\"\"\"\n",
        "#print(nltk.pos_tag(word_tokenize(data_for_slang_2022s)))\n",
        "#print(nltk).pos_tag(word_tokenize(final))\n",
        "#print(nltk).pos_tag(word_tokenize(dataset_slang_2010s)\n",
        "# Stochastic Parts of Speech(POS) Tagging: \n",
        "\n",
        "\n",
        "\n",
        "# Vector similarity is the next step (performed here: )\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data_for_slang_2022s)\n",
        "tfidf_matrix_1 = vectorizer.fit_transform(dataset_slang_2010s)\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix_2 = vectorizer.transform(data_for_slang_2022s)\n",
        "tfidf_matrix13 = tfidf_vectorizer.fit_transform(dataset_slang_2010s)\n",
        "cosine_matrix_similarity = cosine_similarity(tfidf_matrix_2, tfidf_matrix13)\n",
        "print(cosine_matrix_similarity)\n",
        "start = time.time()\n",
        "print(\"Time taken: %s seconds\" % (time.time() - start))\n",
        "\n",
        "\n",
        "cosine_sim = linear_kernel(tfidf_matrix13, tfidf_matrix_2)\n",
        "\n",
        "\"\"\"The techniques for vector similarity are as followed and are cosine similarity, Euclidean distance, Jaccard distance,\n",
        "  word mover’s distance. Cosine similarity is the technique that is being widely used for text similarity.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Decision Function how far off are we from where we wish to be: \n",
        "\n",
        "# Linear Regression model of changes in length of words here:\n",
        "\n",
        "\n",
        "# Linear regression model of changes of meanings(if this is applicable to the project) here: \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bcp2GMdxpEEB",
        "outputId": "e44976e3-8792-47a4-9ee9-f0997fba874a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputting data from 2022s        words used\n",
            "0            stan\n",
            "1           salty\n",
            "2            simp\n",
            "3          cheugy\n",
            "4         zennial\n",
            "5           snack\n",
            "6         sip tea\n",
            "7          sheesh\n",
            "8           slaps\n",
            "9          bussin\n",
            "10       snatched\n",
            "11        sliving\n",
            "12         boujee\n",
            "13            CEO\n",
            "14   sameshitting\n",
            "15            ick\n",
            "16       ate that\n",
            "17          Bible\n",
            "18      Rent-free\n",
            "19  Out of pocket\n",
            "20   caught in 4k\n",
            "21        G.O.A.T\n",
            "\n",
            " outputting data 2010s              words used\n",
            "0               catfish\n",
            "1              bromance\n",
            "2                cougar\n",
            "3               sexting\n",
            "4                 heart\n",
            "5               earworm\n",
            "6     helicopter parent\n",
            "7                selfie\n",
            "8             photobomb\n",
            "9           binge-watch\n",
            "10  first world problem\n",
            "11                deets\n",
            "12                  vom\n",
            "13           humblebrag\n",
            "14          dad dancing\n",
            "15                tweet\n",
            "16           buzzworthy\n",
            "17             unfriend\n",
            "18             jeggings\n",
            "19                 Meme\n",
            "20                emoji\n",
            "21              Freegan \n",
            "\n",
            "This is a vector of numbers    (0, 24)\t1\n",
            "  (1, 15)\t1\n",
            "  (2, 18)\t1\n",
            "  (3, 7)\t1\n",
            "  (4, 27)\t1\n",
            "  (5, 22)\t1\n",
            "  (6, 19)\t1\n",
            "  (6, 25)\t1\n",
            "  (7, 17)\t1\n",
            "  (8, 20)\t1\n",
            "  (9, 4)\t1\n",
            "  (10, 23)\t1\n",
            "  (11, 21)\t1\n",
            "  (12, 3)\t1\n",
            "  (13, 6)\t1\n",
            "  (14, 16)\t1\n",
            "  (15, 9)\t1\n",
            "  (16, 1)\t1\n",
            "  (16, 26)\t1\n",
            "  (17, 2)\t1\n",
            "  (18, 14)\t1\n",
            "  (18, 8)\t1\n",
            "  (19, 12)\t1\n",
            "  (19, 11)\t1\n",
            "  (19, 13)\t1\n",
            "  (20, 5)\t1\n",
            "  (20, 10)\t1\n",
            "  (20, 0)\t1 \n",
            "\n",
            "This is a vector of numbers    (0, 3)\t1\n",
            "  (1, 1)\t1\n",
            "  (2, 4)\t1\n",
            "  (3, 21)\t1\n",
            "  (4, 12)\t1\n",
            "  (5, 8)\t1\n",
            "  (6, 13)\t1\n",
            "  (6, 17)\t1\n",
            "  (7, 20)\t1\n",
            "  (8, 18)\t1\n",
            "  (9, 0)\t1\n",
            "  (9, 25)\t1\n",
            "  (10, 10)\t1\n",
            "  (10, 26)\t1\n",
            "  (10, 19)\t1\n",
            "  (11, 7)\t1\n",
            "  (12, 24)\t1\n",
            "  (13, 14)\t1\n",
            "  (14, 5)\t1\n",
            "  (14, 6)\t1\n",
            "  (15, 22)\t1\n",
            "  (16, 2)\t1\n",
            "  (17, 23)\t1\n",
            "  (18, 15)\t1\n",
            "  (19, 16)\t1\n",
            "  (20, 9)\t1\n",
            "  (21, 11)\t1\n",
            "(22, 28)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Time taken: 4.601478576660156e-05 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The techniques for vector similarity are as followed and are cosine similarity, Euclidean distance, Jaccard distance,\\n  word mover’s distance. Cosine similarity is the technique that is being widely used for text similarity.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1rs9oAkq3MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}